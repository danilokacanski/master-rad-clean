{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T18:14:25.138959Z",
     "start_time": "2025-08-18T18:13:55.607963Z"
    }
   },
   "source": [
    "# Binance 1h OHLCV backfill (CSV only), starting 2020-01-01\n",
    "# - Works for BTC first (can expand later)\n",
    "# - Robust pagination + retries\n",
    "# - Safe resume if CSV already exists\n",
    "\n",
    "import ccxt\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timezone\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "EXCHANGE = \"binance\"\n",
    "SYMBOLS  = [\"BTC/USDT\"]           # start with BTC; add more later\n",
    "TIMEFRAME = \"1h\"                  # fixed per your plan\n",
    "SINCE     = \"2020-01-01T00:00:00Z\"\n",
    "SAVE_DIR  = \"../data/binance\"\n",
    "RATELIMIT_SLEEP = 0.25            # seconds between page requests\n",
    "MAX_RETRIES = 5\n",
    "RETRY_WAIT_BASE = 2.0             # exponential backoff base\n",
    "# ---------------------------------------\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# init exchange (public OHLCV doesn't require keys)\n",
    "ex = getattr(ccxt, EXCHANGE)({\n",
    "    \"enableRateLimit\": True,\n",
    "    \"options\": {\"adjustForTimeDifference\": True}\n",
    "})\n",
    "\n",
    "def iso_to_ms(iso: str) -> int:\n",
    "    return int(parser.isoparse(iso).timestamp() * 1000)\n",
    "\n",
    "def timeframe_ms(tf: str) -> int:\n",
    "    return int(ex.parse_timeframe(tf) * 1000)\n",
    "\n",
    "def read_existing_start(csv_path: str, default_since_iso: str) -> int:\n",
    "    \"\"\"If CSV exists, resume after its last timestamp; else use default SINCE.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return iso_to_ms(default_since_iso)\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, parse_dates=[\"timestamp\"])\n",
    "        if df.empty:\n",
    "            return iso_to_ms(default_since_iso)\n",
    "        last_ts = int(pd.Timestamp(df[\"timestamp\"].max()).tz_convert(\"UTC\").timestamp() * 1000)\n",
    "        # advance by one candle\n",
    "        return last_ts + timeframe_ms(TIMEFRAME)\n",
    "    except Exception:\n",
    "        # If file is corrupted, fall back to default\n",
    "        return iso_to_ms(default_since_iso)\n",
    "\n",
    "def fetch_ohlcv_all(symbol: str, timeframe: str, since_ms: int) -> pd.DataFrame:\n",
    "    \"\"\"Paginate OHLCV safely since `since_ms` → now with retries.\"\"\"\n",
    "    limit = getattr(ex, \"fetch_ohlcv_limit\", lambda tf: 1000)(timeframe) or 1000\n",
    "    rows = []\n",
    "    pbar_desc = f\"{symbol} {timeframe}\"\n",
    "    tf_ms = timeframe_ms(timeframe)\n",
    "\n",
    "    with tqdm(total=0, desc=pbar_desc) as pbar:\n",
    "        while True:\n",
    "            # retry wrapper\n",
    "            for attempt in range(MAX_RETRIES):\n",
    "                try:\n",
    "                    data = ex.fetch_ohlcv(symbol, timeframe=timeframe, since=since_ms, limit=limit)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    wait = RETRY_WAIT_BASE ** attempt\n",
    "                    tqdm.write(f\"[retry {attempt+1}/{MAX_RETRIES}] {e} — sleeping {wait:.1f}s\")\n",
    "                    sleep(wait)\n",
    "            else:\n",
    "                # exhausted retries\n",
    "                break\n",
    "\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            rows.extend(data)\n",
    "            last_ts = data[-1][0]\n",
    "            since_ms = last_ts + tf_ms\n",
    "\n",
    "            # progress feedback\n",
    "            pbar.set_postfix_str(datetime.fromtimestamp(last_ts/1000, tz=timezone.utc).isoformat())\n",
    "            pbar.update(1)\n",
    "\n",
    "            # stop when caught up (exchange returned less than limit)\n",
    "            if len(data) < limit:\n",
    "                break\n",
    "\n",
    "            sleep(RATELIMIT_SLEEP)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"symbol\",\"exchange\"]).set_index(\"timestamp\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"ts\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True)\n",
    "    df = df.drop(columns=[\"ts\"]).set_index(\"timestamp\").sort_index()\n",
    "    df[\"symbol\"] = symbol\n",
    "    df[\"exchange\"] = ex.id\n",
    "    # ensure 1h frequency alignment and drop exact dupes if any\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    return df[[\"open\",\"high\",\"low\",\"close\",\"volume\",\"symbol\",\"exchange\"]]\n",
    "\n",
    "def upsert_csv(csv_path: str, new_df: pd.DataFrame):\n",
    "    \"\"\"Append new rows to CSV (or create). Deduplicate on timestamp.\"\"\"\n",
    "    if new_df.empty:\n",
    "        return 0\n",
    "    if os.path.exists(csv_path):\n",
    "        old = pd.read_csv(csv_path, parse_dates=[\"timestamp\"]).set_index(\"timestamp\")\n",
    "        combo = pd.concat([old, new_df]).sort_index()\n",
    "        combo = combo[~combo.index.duplicated(keep=\"last\")]\n",
    "    else:\n",
    "        combo = new_df\n",
    "    combo.to_csv(csv_path, index=True)\n",
    "    return len(new_df)\n",
    "\n",
    "# -------- run for BTC/USDT from 2020 → now --------\n",
    "for sym in SYMBOLS:\n",
    "    base = sym.replace(\"/\", \"_\")\n",
    "    csv_path = os.path.join(SAVE_DIR, f\"{EXCHANGE}_{base}_{TIMEFRAME}.csv\")\n",
    "\n",
    "    # choose starting point (resume if file exists)\n",
    "    start_ms = read_existing_start(csv_path, SINCE)\n",
    "    start_iso = datetime.fromtimestamp(start_ms/1000, tz=timezone.utc).isoformat()\n",
    "    print(f\"\\nFetching {sym} {TIMEFRAME} from {start_iso} → now\")\n",
    "\n",
    "    df_new = fetch_ohlcv_all(sym, TIMEFRAME, start_ms)\n",
    "    added = upsert_csv(csv_path, df_new)\n",
    "\n",
    "    total = 0\n",
    "    if os.path.exists(csv_path):\n",
    "        total = len(pd.read_csv(csv_path))\n",
    "    print(f\"Saved: {csv_path}  (+{added} new rows, total {total})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching BTC/USDT 1h from 2020-01-01T00:00:00+00:00 → now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BTC/USDT 1h: 50it [00:28,  1.77it/s, 2025-08-18T18:00:00+00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/binance/binance_BTC_USDT_1h.csv  (+49331 new rows, total 49331)\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

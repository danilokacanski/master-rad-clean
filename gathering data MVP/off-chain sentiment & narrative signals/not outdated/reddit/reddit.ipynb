{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-18T10:56:05.312064Z",
     "start_time": "2025-08-18T10:54:45.329707Z"
    }
   },
   "source": [
    "# --- config & imports ---\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pathlib\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Load .env (expects REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT)\n",
    "load_dotenv()\n",
    "CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\", \"crypto-sentiment-scraper/0.1\")\n",
    "\n",
    "# Sanity checks\n",
    "assert CLIENT_ID and CLIENT_SECRET and USER_AGENT, \"Missing Reddit env vars. Set REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT in .env\"\n",
    "\n",
    "# Create API client (read-only)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT,\n",
    ")\n",
    "\n",
    "# --- parameters you can tweak ---\n",
    "SUBS = [\"CryptoCurrency\", \"Bitcoin\"]\n",
    "MODE = \"top\"               # \"hot\" | \"new\" | \"top\"\n",
    "TIME_FILTER = \"day\"        # for .top(): \"hour\" | \"day\" | \"week\" | \"month\" | \"year\" | \"all\"\n",
    "POSTS_PER_SUB = 300        # how many submissions per subreddit\n",
    "FETCH_COMMENTS = True      # set False if you only want posts\n",
    "\n",
    "# Output folder with date stamp\n",
    "out_dir = pathlib.Path(\"reddit_data\") / datetime.now(timezone.utc).strftime(\"%Y%m%d\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def utc_to_iso(ts_utc: float) -> str:\n",
    "    return datetime.fromtimestamp(ts_utc, tz=timezone.utc).isoformat()\n",
    "\n",
    "def get_listing(sr, mode, limit, time_filter):\n",
    "    if mode == \"hot\":\n",
    "        return sr.hot(limit=limit)\n",
    "    elif mode == \"new\":\n",
    "        return sr.new(limit=limit)\n",
    "    elif mode == \"top\":\n",
    "        return sr.top(time_filter=time_filter, limit=limit)\n",
    "    else:\n",
    "        raise ValueError(\"MODE must be one of: hot, new, top\")\n",
    "\n",
    "# ---- scrape posts ----\n",
    "post_rows = []\n",
    "for sub in SUBS:\n",
    "    sr = reddit.subreddit(sub)\n",
    "    print(f\"\\nFetching {MODE} posts from r/{sub} (limit={POSTS_PER_SUB}, time_filter={TIME_FILTER if MODE=='top' else '—'})\")\n",
    "\n",
    "    for s in tqdm(get_listing(sr, MODE, POSTS_PER_SUB, TIME_FILTER), total=POSTS_PER_SUB):\n",
    "        post_rows.append({\n",
    "            \"subreddit\": sub,\n",
    "            \"post_id\": s.id,\n",
    "            \"created_utc\": s.created_utc,\n",
    "            \"created_iso\": utc_to_iso(s.created_utc),\n",
    "            \"author\": getattr(s.author, \"name\", None),\n",
    "            \"title\": s.title,\n",
    "            \"selftext\": s.selftext if hasattr(s, \"selftext\") else None,\n",
    "            \"score\": s.score,\n",
    "            \"upvote_ratio\": getattr(s, \"upvote_ratio\", None),\n",
    "            \"num_comments\": s.num_comments,\n",
    "            \"permalink\": f\"https://reddit.com{s.permalink}\",\n",
    "            \"url\": s.url,                      # external link if any\n",
    "            \"over_18\": s.over_18,\n",
    "            \"stickied\": s.stickied,\n",
    "            \"locked\": s.locked,\n",
    "            \"spoiler\": getattr(s, \"spoiler\", None),\n",
    "            \"flair\": getattr(s, \"link_flair_text\", None),\n",
    "        })\n",
    "        # optional tiny pause to be polite (PRAW handles rate limit; this is extra-safe)\n",
    "        time.sleep(0.05)\n",
    "\n",
    "posts_df = pd.DataFrame(post_rows)\n",
    "# sort newest first\n",
    "if not posts_df.empty:\n",
    "    posts_df = posts_df.sort_values(\"created_utc\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save posts\n",
    "posts_csv = out_dir / f\"reddit_posts_{MODE}_{TIME_FILTER if MODE=='top' else 'na'}.csv\"\n",
    "posts_parquet = out_dir / f\"reddit_posts_{MODE}_{TIME_FILTER if MODE=='top' else 'na'}.parquet\"\n",
    "posts_df.to_csv(posts_csv, index=False)\n",
    "posts_df.to_parquet(posts_parquet, index=False)\n",
    "print(f\"\\nSaved {len(posts_df)} posts → {posts_csv}\")\n",
    "\n",
    "# ---- optionally scrape comments for the collected posts ----\n",
    "if FETCH_COMMENTS and not posts_df.empty:\n",
    "    comment_rows = []\n",
    "    print(\"\\nFetching comments…\")\n",
    "    # modest cap to avoid huge runs; adjust as needed\n",
    "    MAX_COMMENTS_PER_POST = 200\n",
    "\n",
    "    for _, row in tqdm(posts_df.iterrows(), total=len(posts_df)):\n",
    "        submission = reddit.submission(id=row[\"post_id\"])\n",
    "        # replace MoreComments to get full tree (can be slow); use a small limit for speed\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        count = 0\n",
    "        for c in submission.comments.list():\n",
    "            if isinstance(c, MoreComments):\n",
    "                continue\n",
    "            comment_rows.append({\n",
    "                \"subreddit\": row[\"subreddit\"],\n",
    "                \"post_id\": row[\"post_id\"],\n",
    "                \"comment_id\": c.id,\n",
    "                \"created_utc\": c.created_utc,\n",
    "                \"created_iso\": utc_to_iso(c.created_utc),\n",
    "                \"author\": getattr(c.author, \"name\", None),\n",
    "                \"body\": c.body,\n",
    "                \"score\": c.score,\n",
    "                \"permalink\": f\"https://reddit.com{c.permalink}\",\n",
    "                \"is_submitter\": getattr(c, \"is_submitter\", None),\n",
    "            })\n",
    "            count += 1\n",
    "            if count >= MAX_COMMENTS_PER_POST:\n",
    "                break\n",
    "        # micro-sleep between posts\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    comments_df = pd.DataFrame(comment_rows)\n",
    "    if not comments_df.empty:\n",
    "        comments_df = comments_df.sort_values(\"created_utc\", ascending=False).reset_index(drop=True)\n",
    "        comments_csv = out_dir / f\"reddit_comments_{MODE}_{TIME_FILTER if MODE=='top' else 'na'}.csv\"\n",
    "        comments_parquet = out_dir / f\"reddit_comments_{MODE}_{TIME_FILTER if MODE=='top' else 'na'}.parquet\"\n",
    "        comments_df.to_csv(comments_csv, index=False)\n",
    "        comments_df.to_parquet(comments_parquet, index=False)\n",
    "        print(f\"Saved {len(comments_df)} comments → {comments_csv}\")\n",
    "    else:\n",
    "        print(\"No comments collected (empty set).\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching top posts from r/CryptoCurrency (limit=300, time_filter=day)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 59/300 [00:05<00:23, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching top posts from r/Bitcoin (limit=300, time_filter=day)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 51/300 [00:03<00:16, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching top posts from r/ethereum (limit=300, time_filter=day)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:00<00:47,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching top posts from r/ethtrader (limit=300, time_filter=day)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 14/300 [00:01<00:24, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching top posts from r/cryptomarkets (limit=300, time_filter=day)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 27/300 [00:01<00:19, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 153 posts → reddit_data/20250818/reddit_posts_top_day.csv\n",
      "\n",
      "Fetching comments…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [01:06<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3056 comments → reddit_data/20250818/reddit_comments_top_day.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4061a938b6745bca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
